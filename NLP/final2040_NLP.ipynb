{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "# Read the entire file containing song lyrics\n",
    "df = pd.read_csv('songdata.csv')\n",
    "df.head()\n",
    "\n",
    "df = df.dropna(subset = ['text'])\n",
    "df.drop_duplicates(subset = ['text'], keep='first', inplace=True)\n",
    "\n",
    "corpus = sys.argv[1] # first command line arg\n",
    "text = df['text'].str.lower().str.replace('  \\n  \\n','  \\n')\n",
    "text = text.str.replace('  \\n', ' \\n ')\n",
    "text = text.str.replace('\\n\\n', ' \\n ')\n",
    "text = text.str.replace('?','')\n",
    "text = text.str.replace('!','')\n",
    "text = text.str.replace(',', ' ,')\n",
    "text = text.str.replace('  ', ' ')\n",
    "text = text.str.replace('(', '')\n",
    "text = text.str.replace(')', '')\n",
    "print('Corpus length in characters:', len(text))\n",
    "\n",
    "\n",
    "text_in_words = [word for word in text.str.split(' ') ]\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "\n",
    "text_all = \"\"\n",
    "for i in range(0, len(text_in_words)):\n",
    "    if(i % 10 == 0):\n",
    "        for j in range(0,len(text_in_words[i])):\n",
    "            text_all += str(text_in_words[i][j]) + \" \"\n",
    "            \n",
    "    \n",
    "corpus = sys.argv[1] # first command line arg\n",
    "\n",
    "print('Corpus length in characters:', len(text_all))\n",
    "\n",
    "text_all_in_words = [w for w in text_all.split(' ') if w.strip() != '' or w == '\\n']\n",
    "print('Corpus length in words:', len(text_all_in_words))\n",
    "\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_all_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    \n",
    "MIN_WORD_FREQUENCY=450\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)  \n",
    "    \n",
    "words = set(text_all_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "\n",
    "\n",
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "SEQUENCE_LEN = 10\n",
    "STEP = 1\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(text_all_in_words) - SEQUENCE_LEN, STEP):\n",
    "    # Only add sequences where no word is in ignored_words\n",
    "    if len(set(text_all_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(text_all_in_words[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_all_in_words[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored+1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=10):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "    \n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words, percentage_test=10)  \n",
    "  \n",
    "    \n",
    "    \n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)    \n",
    "    \n",
    "    \n",
    "    \n",
    "def on_epoch_end_orig(epoch, logs):\n",
    "    print()\n",
    "    if(epoch > 45):\n",
    "        print('----- Generating text after Epoch: %d\\n' % epoch)\n",
    "        for diversity in [0.2, 0.5, 1.0]:\n",
    "            \n",
    "            print('----- Diversity:', diversity, ' -----')\n",
    "            generated = ['life','is']\n",
    "            sentence = generated\n",
    "            for i in range(100):\n",
    "                \n",
    "                x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "                for t, word in enumerate(sentence):\n",
    "                    x_pred[0, t, word_indices[word]] = 1.\n",
    "        \n",
    "                preds = model_orig.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_word = indices_word[next_index]\n",
    "                generated.append(next_word)\n",
    "                sentence = sentence[1:] + [next_word]\n",
    "                sys.stdout.write(next_word)\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            print()\n",
    "\n",
    "print_callback_orig = LambdaCallback(on_epoch_end=on_epoch_end_orig)    \n",
    "    \n",
    "    \n",
    "    \n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y    \n",
    "\n",
    "        \n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "print('Building the LSTM Model')\n",
    "\n",
    "model_orig = Sequential()\n",
    "model_orig.add(LSTM(128, input_shape=(SEQUENCE_LEN, len(words))))\n",
    "model_orig.add(Dense(len(words)))\n",
    "model_orig.add(Activation('softmax'))\n",
    "model_orig.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01), metrics=['accuracy'])\n",
    "model_orig.summary()\n",
    "\n",
    "\n",
    "history_orig = model_orig.fit(generator(sentences, next_words, batch_size=128),\n",
    "                              steps_per_epoch=int(len(sentences)/batch_size) + 1,\n",
    "                              epochs=50,\n",
    "                              callbacks=[print_callback_orig],\n",
    "                              validation_data=generator(sentences_test, next_words_test, batch_size), \n",
    "                              validation_steps=int(len(sentences_test)/batch_size) + 1)\n",
    "\n",
    "print(\"LSTM Network Trained\")\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history_orig.history['loss']\n",
    "val_loss = history_orig.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# diversity is used by the sample function, that will randomly select\n",
    "# the next most probable character from the softmax output \n",
    "\n",
    "print('----- Generating text -----')\n",
    "for diversity in [0.2, 0.5, 1.0]:\n",
    "    \n",
    "    print()\n",
    "    sentence = ['life','is']\n",
    "    original = \" \".join(sentence)\n",
    "    generated = sentence\n",
    "    window = sentence\n",
    "    finalText = ''\n",
    "    print('----- Diversity:', diversity, ' -----\\n')\n",
    "\n",
    "    for i in range(50):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "        for t, word in enumerate(window):\n",
    "            x_pred[0, t, word_indices[word]] = 1.0\n",
    "        \n",
    "        preds = model_orig.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_word = indices_word[next_index]\n",
    "        finalText += \" \"+next_word\n",
    "        window = window[1:] + [next_word]\n",
    "  \n",
    "    print(original + finalText)\n",
    "\n",
    "print('----- Text generation complete! -----')\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(SEQUENCE_LEN, len(words))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(len(words)+50, activation = 'softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(len(words), activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    print()\n",
    "    if(epoch > 95):\n",
    "        print('----- Generating text after Epoch: %d\\n' % epoch)\n",
    "        for diversity in [0.2, 0.5, 1.0]:\n",
    "            \n",
    "            print('----- Diversity:', diversity, ' -----')\n",
    "            generated = ['life','is']\n",
    "            sentence = generated\n",
    "            for i in range(100):\n",
    "                \n",
    "                x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "                for t, word in enumerate(sentence):\n",
    "                    x_pred[0, t, word_indices[word]] = 1.\n",
    "        \n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_word = indices_word[next_index]\n",
    "                generated.append(next_word)\n",
    "                sentence = sentence[1:] + [next_word]\n",
    "                sys.stdout.write(next_word)\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            print()\n",
    "            \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=5)\n",
    "    \n",
    "    \n",
    "history = model.fit_generator(generator(sentences, next_words, batch_size=128),\n",
    "                              steps_per_epoch=int(len(sentences)/batch_size) + 1,\n",
    "                              epochs=100,\n",
    "                              callbacks=[print_callback,early_stopping],\n",
    "                              validation_data=generator(sentences_test, next_words_test, batch_size),              \n",
    "                              validation_steps=int(len(sentences_test)/batch_size) + 1)    \n",
    "    \n",
    "    \n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "# diversity is used by the sample function, that will randomly select\n",
    "# the next most probable character from the softmax output \n",
    "\n",
    "print('----- Generating text -----')\n",
    "for diversity in [0.2, 0.5, 1.0]:\n",
    "    \n",
    "    print()\n",
    "    sentence = ['life','is']\n",
    "    original = \" \".join(sentence)\n",
    "    generated = sentence\n",
    "    window = sentence\n",
    "    finalText = ''\n",
    "    print('----- Diversity:', diversity, ' -----\\n')\n",
    "\n",
    "    for i in range(50):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "        for t, word in enumerate(window):\n",
    "            x_pred[0, t, word_indices[word]] = 1.0\n",
    "        \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_word = indices_word[next_index]\n",
    "        finalText += \" \"+next_word\n",
    "        window = window[1:] + [next_word]\n",
    "  \n",
    "    print(original + finalText)\n",
    "\n",
    "print('----- Text generation complete! -----')\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(GRU(32, input_shape=(SEQUENCE_LEN, len(words))))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(Dense(len(words)+50, activation = 'softmax'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(len(words), activation = 'softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "\n",
    "\n",
    "def on_epoch_end1(epoch, logs):\n",
    "    print()\n",
    "    if(epoch > 95):\n",
    "        print('----- Generating text after Epoch: %d\\n' % epoch)\n",
    "        for diversity in [0.2, 0.5, 1.0]:\n",
    "            \n",
    "            print('----- Diversity:', diversity, ' -----')\n",
    "            generated = ['life','is']\n",
    "            sentence = generated\n",
    "            for i in range(100):\n",
    "                \n",
    "                x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "                for t, word in enumerate(sentence):\n",
    "                    x_pred[0, t, word_indices[word]] = 1.\n",
    "        \n",
    "                preds = model1.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_word = indices_word[next_index]\n",
    "                generated.append(next_word)\n",
    "                sentence = sentence[1:] + [next_word]\n",
    "                sys.stdout.write(next_word)\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            print()\n",
    "            \n",
    "print_callback1 = LambdaCallback(on_epoch_end=on_epoch_end1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=5)\n",
    "\n",
    "\n",
    "\n",
    "history1 = model1.fit_generator(generator(sentences, next_words, batch_size=128),\n",
    "                              steps_per_epoch=int(len(sentences)/batch_size) + 1,\n",
    "                              epochs=100,\n",
    "                              callbacks=[print_callback1,early_stopping],\n",
    "                              validation_data=generator(sentences_test, next_words_test, batch_size),              \n",
    "                              validation_steps=int(len(sentences_test)/batch_size) + 1)\n",
    "\n",
    "\n",
    "\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# diversity is used by the sample function, that will randomly select\n",
    "# the next most probable character from the softmax output \n",
    "\n",
    "print('----- Generating text -----')\n",
    "for diversity in [0.2, 0.5, 0.8, 1.0]:\n",
    "    \n",
    "    print()\n",
    "    sentence = ['life','is']\n",
    "    original = \" \".join(sentence)\n",
    "    generated = sentence\n",
    "    window = sentence\n",
    "    finalText = ''\n",
    "    print('----- Diversity:', diversity, ' -----\\n')\n",
    "\n",
    "    for i in range(50):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "        for t, word in enumerate(window):\n",
    "            x_pred[0, t, word_indices[word]] = 1.0\n",
    "        \n",
    "        preds = model1.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_word = indices_word[next_index]\n",
    "        finalText += \" \"+next_word\n",
    "        window = window[1:] + [next_word]\n",
    "  \n",
    "    print(original + finalText)\n",
    "\n",
    "print('----- Text generation complete! -----')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data generator for fit and evaluate\n",
    "def generator_embedding(sentence_list, next_words_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.bool)\n",
    "        y = np.zeros((batch_size), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = next_words_list[index % len(sentence_list)]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "        \n",
    "        \n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(words), 64))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(GRU(64))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(len(words)))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "\n",
    "\n",
    "def on_epoch_end2(epoch, logs):\n",
    "    print()\n",
    "    if(epoch > 45):\n",
    "        print('----- Generating text after Epoch: %d\\n' % epoch)\n",
    "        for diversity in [0.2, 0.5, 0.8, 1.0]:\n",
    "            \n",
    "            print('----- Diversity:', diversity, ' -----')\n",
    "            generated = ['life','is']\n",
    "            sentence = generated\n",
    "            for i in range(100):\n",
    "                \n",
    "                x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "                for t, word in enumerate(sentence):\n",
    "                    x_pred[0, t, word_indices[word]] = 1.\n",
    "        \n",
    "                preds = model2.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_word = indices_word[next_index]\n",
    "                generated.append(next_word)\n",
    "                sentence = sentence[1:] + [next_word]\n",
    "                sys.stdout.write(next_word)\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            print()\n",
    "            \n",
    "print_callback2 = LambdaCallback(on_epoch_end=on_epoch_end2)\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=10)\n",
    "\n",
    "\n",
    "\n",
    "history2 = model2.fit_generator(generator_embedding(sentences, next_words, batch_size=128),\n",
    "                              steps_per_epoch=int(len(sentences)/batch_size) + 1,\n",
    "                              epochs=100,\n",
    "                              callbacks=[print_callback2,early_stopping],\n",
    "                              validation_data=generator_embedding(sentences_test, next_words_test, batch_size),              \n",
    "                              validation_steps=int(len(sentences_test)/batch_size) + 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# diversity is used by the sample function, that will randomly select\n",
    "# the next most probable character from the softmax output \n",
    "\n",
    "print('----- Generating text -----')\n",
    "for diversity in [0.2, 0.4,0.5,0.6,0.7, 0.8,0.9, 1.0]:\n",
    "    \n",
    "    print()\n",
    "    sentence = ['i','love']\n",
    "    original = \" \".join(sentence)\n",
    "    generated = sentence\n",
    "    window = sentence\n",
    "    finalText = ''\n",
    "    print('----- Diversity:', diversity, ' -----\\n')\n",
    "\n",
    "    for i in range(50):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "        for t, word in enumerate(window):\n",
    "            x_pred[0, t, word_indices[word]] = 1.0\n",
    "        \n",
    "        preds = model_orig.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_word = indices_word[next_index]\n",
    "        finalText += \" \"+next_word\n",
    "        window = window[1:] + [next_word]\n",
    "  \n",
    "    print(original + finalText)\n",
    "\n",
    "print('----- Text generation complete! -----')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
